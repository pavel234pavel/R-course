---
title: "Семинар 4. Кластерный анализ"
editor_options:
  chunk_output_type: console
date: "Октябрь 1, 2018"
output:
  html_document:
    keep_md: no
    number_sections: yes
    toc: yes
  word_document:
    toc: yes
  pdf_document:
    toc: yes
lang: ru-RU
---


Подключаем пакеты!

```{r, "setup-chunk", message=FALSE, warning=FALSE}
library(tidyverse) # обработка данных, графики...
library(skimr) # описательные статистики
library(rio) # импорт фантастического количества форматов данных

library(cluster) # кластерный анализ
library(factoextra) # визуализации kmeans, pca
library(dendextend) # визуализация дендрограмм

library(corrplot) # визуализация корреляций

library(broom) # метла превращает результаты оценивания моделей в таблички

library(naniar) # визуализация пропущенных значений
library(visdat) # визуализация пропущенных значений

library(patchwork) # удобное расположение графиков рядом

library(nycflights13) # данные о полётах в Нью-Йорке
```



# Основные классы данных

В R несколько разных классов данных:

* вектора
* таблички
* матрицы
* списки
* ещё куча всего

### Векторы

Создадим несколько векторов с пропущенными значениями!
```{r}
a <- c(5, 6, 7, NA, 8)
b <- 11:15
b
```

- Что находится в векторе `b`?

С числовыми векторами работают школьные функции:
```{r}
max(b)
max(a)
max(a, na.rm = TRUE)
mean(a)
mean(a, na.rm = TRUE)
```

- Чем отличаются функции `max(a)` и `max(a, na.rm = TRUE)`?


### Таблички

Таблицы можно создавать самим c помощью функции `tibble()`.

```{r}
mx <- tibble(ID = c("A", "B", "C", "D", "E"),
           x = c(1, 2, 4, 1, 8))
mx
```

* Упражнение 1.

Создайте таблицу `my`, с двумя столбцами: `ID` и `y`, и пятью значениями в каждом.
Столбец `y` можно заполнить числами произвольно.

```{r}
# my <- tibble(ID = c("A", "B", "C", "D", "E"),
#             y = c(___))
# my
```

### Матрицы

Матрицы очень похожи на таблицы.
Их основное отличие в том, что в них могут быть только числа.
С матрицами работают люди, которым нужна ну очень высокая скорость.
Мы будем работать только с таблицами.

```{r}
my_matrix <- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2)
my_matrix
class(my_matrix)
t(my_matrix)
```

А если нам вдруг встретится злобная, но ценная матрица,
мы её тут же преобразуем в таблицу.

```{r}
skim(my_matrix) # не работает :(
my_tibble <- as_tibble(my_matrix)
skim(my_tibble)
```


### Списки

Список - это чулан, в котором может быть полно ценных вещей.
В списке может храниться всё что угодно.
Как правило в списках хранятся результаты оценивания сложных моделей.

```{r}
my_list <- list(kreks = "Привет!", peks = c(5, 6, NA), feks = diamonds)
my_list$kreks
my_list$peks
attributes(my_list)
```

Узнать, что лежит в чулане-списке, можно либо с помощью функции `attributes`,
либо нажав на значок лупы во вкладке `Environment` в Rstudio.
Обычно вкладка `Environment` справа вверху.

# Соединение таблиц

Таблицы можно соединять! Существует четыре способа сделать это:
- `left_join(x, y, by = "colname")` сохраняет все наблюдения в таблице `x`
- `right_join(x, y, by = "colname")` сохраняет все наблюдения в таблице `y`
- `full_join(x, y, by = "colname")` сохраняет наблюдения из обеих таблиц
- `inner_join(x, y, by = "colname")` объединяет только те строки обеих таблиц, в которых нет пропущенных наблюдений

* Упражнение 2.

Экспериментируем!

```{r}
# left_join(mx, my, by = "ID")
# right_join(mx, my, by = "ID")
# inner_join(mx, my, by = "ID")
# full_join(mx, my, by = "ID")
```

- Сколько наблюдений получается в объединённой таблице в каждом случае?

* Упражнение 3.

Нужно объединить таблицы `flights` и `weather` о вылетах из Нью-Йорка сразу по нескольким столбцам!

- 'Бросьте взгляд' на обе таблицы. Какие столбцы совпадают?
```{r}
# glimpse(___)
# ___
```

- Присоедините к таблице `flights` таблицу `weather` с помощью функции  `left_join()`.
В качестве аргумента `by` используйте вектор из названий общих столбцов: `year`, `month`, `day`, `hour` и `origin`.
- Сколько наблюдений в объединённой таблице?

```{r}
# left <- left_join(___, ____, by = c("year", "month", "day", "hour", "origin"))
# glimpse(___)
```

- Повторите соединение двух таблиц, но используя `inner_join()`. Сколько наблюдений в получившейся таблице?
- В чём разница `left_join` и `inner_join`?

```{r}
# inner <- inner_join(___, ___, ___)
# ___
```

# Длинные и широкие таблицы


Таблицы бывают длинными и широкими!
При поступлении нового наблюдения длинная растёт в длину, а широкая может и в ширину :)

Возьмём квартальные данные [Росстата](http://www.gks.ru/free_doc/new_site/vvp/kv/tab35.htm)
о ВВП по источникам доходов.

```{r}
gdp <- import("data/gdp.xls")
glimpse(gdp)
gdp
```

Обнаружив неудачное название переменной переименуем её сразу!
```{r}
gdp <- rename(gdp, indicator = X__1)
```

Чтобы превратить широкую таблицу в длинную, нужна функция `gather()` из пакета `tidyr`.

В длинной таблице мы создадим два новых столбца:
  - столбец `date` с названиями старых столбцов
  - столбец `quantity` со значениями таблицы

Если столбец `indicator` нужно оставить как есть, упоминаем его с минусом!

```{r}
long_gdp <- gather(gdp, key = "date", value = "quantity", -indicator)
glimpse(long_gdp)
```

Создадим типичную длинную таблицу!
Например, для каждого качества огранки `cut` и цвета `color` посчитаем медиану цены:
```{r}
diamonds_med <- group_by(diamonds, cut, color) %>%
  summarise(med_price = median(price))
diamonds_med
```

А теперь превратим её в широкую:
```{r}
diamonds_wide <- spread(diamonds_med, key = "color", value = "med_price")
diamonds_wide
```
Здесь тупому компьютеру умный Homo Sapiens укажет:
  - какая переменная станет столбцами новой таблицы: `color`
  - какая переменная пойдёт внутрь таблицы: `med_price`

Не упомянутые переменные останутся строками новой широкой таблицы.


* Упражнение 4.

Из данных про воздух в Нью-Йорке `airquality` сделайте длинную таблицу.
В качестве сохраняемых столбцов возьмите переменные `Month` и `Day`.


```{r}
# air_long <- gather(___, key = "indicator", value = "value", -Month, -___)
# air_long
```

- Как называются столбцы в длинной таблице? Что в них находится?

А из получившейся длинной таблицы восстановите исходную!

```{r}
# air_wide <- spread(___, key = "___", value = "value")
# air_wide
```



# Работа с пропущенными данными :)


Команда `skim()` для описательных статистик показывает число пропущенных наблюдений в каждой перменной.
Оказывается, их тоже можно визуализировать!
Для этого будем использовать пакеты `naniar` и `visdat`.
Посмотрим на пропущенные значения в сгенирированных данных `typical_data` командой `vis_miss()`.
Сначала не будем добавлять аргументы, а затем сгруппируем пропуски с помощью иерархической кластеризации аргументом `cluster = TRUE`.

```{r}
skim(typical_data)
vis_miss(typical_data)
vis_miss(typical_data, cluster = TRUE)
```


* Упражнение 5.

Визуализируйте пропуски во встроенных данных о воздухе в Нью-Йорке `airquality`, сгруппировав их по кластерам.

```{r}
skim(airquality)
# vis_miss(___, cluster = ___)
```

- Какие признаки содержат пропущенные наблюдения?
- Какова доля пропусков в данных `airquality`?


Команда `gg_miss_var()` также позволяет посмотореть на количество пропущенных значений.
По умолчанию пропуски считаются в абсолютных значениях, заказать проценты можно аргументом `show_pct = TRUE`.

```{r}
gg_miss_var(typical_data)
gg_miss_var(typical_data, show_pct = TRUE) # число пропусков в процентах
```

Ещё больше красивых визуализаций — в виньетке пакета [`naniar`](https://cran.r-project.org/web/packages/naniar/vignettes/getting-started-w-naniar.html).


Теперь, зная врага в лицо, приступим к борьбе с ним!
Самый простой способ — удалить все строки данных, в которых есть пропуски.
Сделаем это командой `drop_na()`.

```{r}
typical_removed <- drop_na(typical_data)
skim(typical_removed)
```

- Сколько наблюдений осталось?

Команда `replace_na` из пакета `tidyr` позволяет заполнять пропуски любым числом, в том числе средним или медианой.
Заполним пропущенные значения в переменной `IQ` минимальным значением, доход `Income` — медианным,  `ID` — числом, а возраст `Age` — средним.
Эти пожелания передадим аргументу `replace` списком.

```{r, eval=FALSE}
typical_replaced <- replace_na(data = typical_data,
  replace = list(IQ =  min(typical_data$IQ, na.rm = TRUE),
  Income = median(typical_data$Income, na.rm = TRUE),
  ID = -1,
  Age = mean(typical_data$Age, na.rm = TRUE)))
skim(typical_replaced)
```


* Упражнение 7.

- Удалите все наблюдения с пропусками из данных `airquality`. Сколько наблюдений осталось?
- Заполните пропуски в данных. Для переменной `Ozone` — средним значением, а для `Solar.R` — медианным.
- Проверьте, что всё получилось :)

```{r}
# air_removed <- drop_na(___)
# skim(___)

# air_replaced <- replace_na(data = ___,
#  replace = list(Ozone = mean(airquality$___, na.rm = ___),
#  Solar.R = median(___, na.rm = ___)))
# skim(___)
```


# Кластеризация k-means

Для примера возьмём данные по потреблению протеинов Европе из книги [Practial Machine Learning Cookbook](https://github.com/PacktPublishing/Practical-Machine-Learning-Cookbook/blob/master/Chapter%2003/Data/Europenaprotein.csv).
Сначала, как всегда, загрузим их и посмотрим описательные статистики.

```{r}
protein <- import("data/Europenaprotein.csv")
skim(protein)
```

- Есть ли в данных пропущенные наблюдения?

## Масштабирование переменных

Для того, чтобы сравнивать переменные с разными единицами измерения, их масштабируют:
вычитают среднее и делят на оценку стандартного отклонения.

\[
x_i^* = (x_i - \bar x) / \hat \sigma_x,
\]
где $\bar x = \frac{x_1 + x_2 + \ldots + x_n}{n}$, а
\[
\hat\sigma_x = \frac{(x_1 - \bar x)^2 + \ldots + (x_n - \bar x)^2}{n-1}.
\]


Отмасштабируем данные с помощью встроенной функции `scale()`.
Поскольку она может работать только с числами, первый столбец `Country` ей передавать не нужно.
Результат сохраним в таблице `protein_stand`.


```{r}
protein_stand <- mutate_if(protein, is.numeric, ~ as.vector(scale(.)))
skim(protein_stand)
```

Дополнение в виде функции `as.vector` нужно потому, что функция `scale` возвращает матрицу,
а каждый столбец должен быть вектором :)

- Посмотрите на описательные статистики. Что стало со средним значением и стандартным отклонением переменных?


* Упражнение 8.

Хитрая исследовательница Даша хочет возвести в квадрат сразу все числовые столбцы
в наборе данных `diamonds`.
Помогите Даше возвести столбцы в квадрат!

```{r}
# diamonds2 <- mutate_if(___, ___, ~ .^2)
```



* Упражнение 9.

- Посмотрите на описательные статистики встроенного набора данных об арестах в США.
- Отмасштабируйте данные и сохраните их в таблицу `usa_stand`.
- Проверьте, что всё получилось :)

```{r}
usa <- USArrests
# skim(___)

# usa_stand <- ___(___ , ___, ~ as.vector(scale(.)))
# ___
```

Выполним кластеризацию методом k-средних с помощью функции `kmeans`.
Название страны не используется для кластеризации, но нужно для меток на графиках.
Поэтому мы уберем столбец `Country` из набора данных и превратим его в метки строк.

В качестве аргументов укажем отмасштабированные данные `protein_no_country` и количество кластеров `centers`.
Пока мы не знаем, как выбирать оптимальное количество кластеров, поэтому предположим, что их три.
Сохраним результат этого действия в список `k_means_protein`.

```{r}
protein_no_country <- protein_stand %>% column_to_rownames(var = "Country")
k_means_protein <- kmeans(protein_no_country, centers = 3)
k_means_protein
```

В списке `k_means_protein` лежит куча разной информации!
Посмотрим на содержимое списка `k_means_protein` командой `attributes()`.
```{r}
attributes(k_means_protein)
```

Помните также, что в Rstudio можно кликнуть на лупу справа от `k_means_protein` во вкладке `Environment`.

Теперь, зная, что искать, мы можем посмотреть, например,
на координаты центра кластеров или количество объектов в каждом из них.

```{r}
k_means_protein$centers
k_means_protein$cluster
k_means_protein$size
```

Другой способ структурировать вывод `kmeans` — использовать команду `tidy` из пакета `broom`.

```{r}
tidy(k_means_protein)
```

Первые девять неназванных переменных — центры кластеров по каждой переменной.


* Упражнение 10.

Кластеризуйте отмасштабированные данные по арестам в США.
В выборе числа кластеров доверьтесь интуиции.

```{r}
# k_means_usa <- kmeans(___, centers = ___)
```


Осталось только визуализировать результаты!
Для этого будем использовать команду `fviz_cluster()` из пакета `factoextra`.
Её аргументы — результат кластеризации `k_means_protein`,
исходные данные и ещё куча настроек вроде размера точек и цвета наблюдений-выбросов.
Мы только попросим выделять цветом кластеры по их границам и укажем аргумент `ellipse.type = 'convex'`.

```{r, echo=FALSE}
fviz_cluster(object = k_means_protein, data = protein_no_country,
             ellipse.type = "convex")
```

* Упражнение 11.

Визуализируйте результаты кластеризации данных по арестам `k_means_usa`.

```{r}
# fviz_cluster(___, data = ___, ellipse.type = "convex")
```

- Пересекаются ли кластеры?
- Разумно ли изменить количество кластеров?


Как понять, сколько кластеров брать оптимально?
Один из способов сделать это — воспользоваться командой `fviz_nbclust` из пакета `factoextra`.


```{r}
g1 <- fviz_nbclust(protein_no_country, kmeans, method = "wss") +
  labs(subtitle = "Elbow method")
g1

g2 <- fviz_nbclust(protein_no_country, kmeans, method = "silhouette") +
  labs(subtitle = "Silhouette method")
g2

g3 <- fviz_nbclust(protein_no_country, kmeans, method = "gap_stat") +
  labs(subtitle = "Gap statistic method")
g3
```

С помощью хитрого пакета Томаса располагать графики легко!
Попробуйте!

```{r}
(g1 + g2) / g3
g1 + g2 + g3
g1 + (g2 / g3)
```

- В чём разница?
- Как расположит графики команда `g1 / (g2 + g3)`?


* Упражнение 12.

Проверьте, совпадает ли выбранное вами число кластеров для `usa_stand` с оптимальным.
Изобразите все три диаграммы вместе: первые две вместе наверху, вторую отеднльно внизу.

```{r}
# p1 <- fviz_nbclust(___, kmeans, method = "wss") +
#  labs(subtitle = "Elbow method")
# p2 <- fviz_nbclust(___, ___, method = "silhouette") +
#  labs(subtitle = "Silhouette method")
# p3 <- fviz_nbclust(___, ___, method = "gap_stat") +
#  labs(subtitle = "Gap statistic method")
# (___ + ___) / ___
```


Метки кластерам легко добавить к исходным данным:

```{r}
protein_plus <- mutate(protein, cluster = k_means_protein$cluster)
glimpse(protein_plus)
```

# Иерархическая кластеризация

Другой способ разбить данные на группы — иерархическая кластеризация.
Но, в отличие от метода k-средних, она работает с матрицей расстояний,
поэтому первым делом посчитаем её!
Для этого будем использовать функцию `dist()`.
Передадим ей стандартизированные данные и укажем явно, как считать расстояния с помощью аргумента `method`.
О всех остальных опциях можно узнать в справке.

```{r}
protein_dist <- dist(protein_no_country, method = "euclidian")
```

Расстояния тоже можно визуализировать!
Сделаем это командой `fviz_dist` из пакета `factoextra`.

```{r}
fviz_dist(protein_dist)
```

* Упражнение 13.

Посчитайте матрицу расстояний для таблицы `usa_stand` и визуализируйте её.

```{r}
# usa_dist <- dist(___, method = "euclidian")
# fviz_dist(___)
```

Полученную матрицу расстояний можно передадать функции `hclust()`, которая кластеризует данные.
Однако в пакете `factoextra` есть функция `hcut()`, которая работает с исходными данными.
Будем использовать её и попросим выделить четыре кластера в аргументе `k`.


```{r}
protein_hcl <- hcut(protein_no_country, k = 4)
```

С помощью функции `fviz_dend` визуализируем результат кластеризации.
Укажем несколько аргументов, чтобы сделать дендрограмму красивее,
а полный перечень найдётся в справке.

```{r}
fviz_dend(protein_hcl,
          cex = 0.5, # размер подписи
          color_labels_by_k = TRUE) # цвет подписей по группам
```

Выявленные кластеры можно добавить к исходным данным!
```{r}
protein_plus2 <- mutate(protein, cluster = protein_hcl$cluster)
glimpse(protein_plus2)
```


* Упражнение 14.

- Сделайте иерархическую кластеризацию с четыремя группами на данных об арестах.

- Визуализируйте результат кластеризации и сделайте подписи цветными.

```{r}
# usa_hcl <- hcut(___, k = ___)
# fviz_dend(___,
#          cex = 0.5, # размер подписи
#          color_labels_by_k = ___) # цвет подписей по группам
```

Иерархичская кластеризация полезна и для визуализаций корреляций.
Если в функции `corrplot()` из одноимённого пакета указать аргумента `order = "hclust"`,
то мы получим сгруппированные по кластерам переменные.
Для красоты добавим ещё один аргумент — `addrect = 3`.
Он обведёт прямоугольниками указанное число кластеров.

```{r}
protein_cor <- cor(protein_no_country)
corrplot(protein_cor, order = "hclust", addrect = 3)
```

* Упражнение 15.

Визуализируйте корреляции в данных об арестах `usa` и сгруппируйте их по двум кластерам.
Замените кружочки на квадраты, передав аргументу `method` значение `shade`.

```{r}
# usa_cor <- cor(___)
# corrplot(___, order = ___, addrect = ___, ___ = ___)
```

* Упражнение 16.

Добавьте к исходным данным `usa` кластеры, полученные с помощью иерархической кластеризации:
```{r}
# usa_plus2 <- mutate(___, cluster = ___)
# glimpse(___)
```

Визуализации [кластеров в известных наборах данных] (https://cran.r-project.org/web/packages/dendextend/vignettes/Cluster_Analysis.html)

Ура! :)
